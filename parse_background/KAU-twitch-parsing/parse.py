#!/usr/bin/python3

from printProgressBar import printProgressBar

import pandas as pd
import os
from multiprocessing import Pool
import timeit
import tqdm
import argparse

ap = argparse.ArgumentParser()
ap.add_argument("-w"     , required = False, default = 10 , type = int, help = "number of workers (multiprocessing)")
args = vars(ap.parse_args())

'''
TODO: progressbar
'''

# the usable captures
DIR_INPUT = "twitch/usable_captures_h5/"
# for the parsed captures
DIR_OUTPUT = "twitch/parsed_captures/"
# 1000000000 ns = 1 sec
NANO_SEC_PER_SEC = 1000000000
# How much of the header to remove (to fit the noise with the web traffic)
HEADER = 40
# for storing the result as h5
KEY = "df"
TIME_INDEX            = 1
SENDER_RECEIVER_INDEX = 2
SIZE_INDEX            = 3

DF_COLUMNS = ['time', 'direction', 'size']

def main():
    '''
    Parse background data generated by rds-collect

    Assumption about the input:
    * the files has been converted to hdf5 files (run log_2_h5.py)
    * that the unusable files is removed (run rm_broken_captures.py)
    * that the captured data starts its captures from the time 0
    '''

    print("Start parsing (make take some time)")

    # is used to get the direction of each packet
    ipHost = '10.88.0.9'

    # clean old results if their is any
    if os.path.exists(DIR_OUTPUT):
        shutil.rmtree(DIR_OUTPUT)
    os.mkdir(DIR_OUTPUT)

    # the capture files that will be parsed
    input_files = os.listdir(DIR_INPUT)
    input_files.sort()
    
    input = []
    for curr_file in input_files:
        input.append((curr_file, ipHost))

    start_time = timeit.default_timer()
    p = Pool(args['w'])

    p.starmap(parse_file, input)

    end_time = timeit.default_timer()
    print(f"runtime for parsing (sec): {end_time - start_time}")
    print("Have saved the parsed results, ending the program")
    return


def parse_file(file, ipHost):
    # should start at 0 for each file
    prev_time = 0
    # to store the parsed file
    df_parsed = pd.DataFrame(columns = DF_COLUMNS)
    # Dictionary to append the results for each row for a file
    dictionary_parsed = {
        'time': [],
        'direction': [],
        'size': []
    }

    filename = os.fsdecode(file)
    # get the data from the current file
    path = DIR_INPUT + filename
    df_unsorted   = pd.read_hdf(path, key=KEY)

    df = df_unsorted.sort_values(by='time')

    capture_len = 0
    for row in df.itertuples():
        capture_len += 1
        # flag to check if the packet is broken, so it can be skipped
        broken = False

        # convert from timestamp in sec, to duration form last packet in ns
        if not row[TIME_INDEX]:
            broken = True
            continue
        else:
            # get the duration (in ns) between this packet, and the one before it
            parsed_time_float_sec = row[TIME_INDEX] - prev_time
            parsed_time_float_ns  = parsed_time_float_sec * NANO_SEC_PER_SEC
            parsed_time           = round(parsed_time_float_ns)

            # Some packets are in the wrong order, 
            if parsed_time < 0:
                broken = True
                continue
            elif parsed_time == 0:
                parsed_time = 1

        sender_receiver = str(row[SENDER_RECEIVER_INDEX]).split(",")
        # if no or only one IP address, skip this packet
        if len(sender_receiver) < 2:
            broken = True
            continue
        else:
            sender          = sender_receiver[0]
            receiver        = sender_receiver[1]

            # get direction
            if sender == "":
                broken = True
                continue
            elif sender == ipHost:
                parsed_direction = "sb"
            elif receiver == ipHost:
                parsed_direction = "rb"
            else:
                sender_start_ip = sender.split('.')
                if sender_start_ip[0] == '10':
                    ipHost = sender_start_ip[0]
                    parsed_direction = "sb"
                else:
                    ipHost = sender_start_ip[1]
                    parsed_direction = "rb"

        # get size
        try:
            parsed_size = int(row[SIZE_INDEX]) - HEADER
        except:
            broken = True
            continue

        if parsed_size <= 0:
            broken = True
            continue

        if not broken:
            dictionary_parsed['time'].append(parsed_time)
            dictionary_parsed['direction'].append(parsed_direction)
            dictionary_parsed['size'].append(parsed_size)

            # update time for the packet before (in sec as float)
            prev_time = row[TIME_INDEX]

    # have parsed the whole file, store the result
    df_parsed = pd.DataFrame(dictionary_parsed)

    df_file_name = DIR_OUTPUT + filename.rsplit('.', 1)[0] + '.h5'
    df_parsed.to_hdf(df_file_name, mode = "w", key = KEY) 

    return


# run main 
if __name__=="__main__":
    main()